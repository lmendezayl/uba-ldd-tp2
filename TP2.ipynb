{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUFNWmtPrSFNZZauPO1/c1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmendezayl/uba-ldd-tp2/blob/main/TP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trabajo Práctico N°2\n",
        "\n",
        "Integrantes:\n",
        "- Lautaro Evaristo Mendez\n",
        "- Franco Zalazar\n",
        "- Luca Petrarca"
      ],
      "metadata": {
        "id": "P9thIV0bDZA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*- In god we trust, all others must bring data.*\n",
        "\n",
        "*W. E. Demings*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TozHnBq8pUny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering [4 pts.]\n"
      ],
      "metadata": {
        "id": "xwz8P-rpDCbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Nuestro primer objetivo es realizar un agrupamiento de las noticias de deportes por temas. En este caso, no tenemos una respuesta correcta que queremos predecir. Queremos realizar un agrupamiento y ver si podemos identificar temas en los grupos que obtenemos.\n"
      ],
      "metadata": {
        "id": "d4Aulx5ypKDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Bag-of-words** Queremos generar primero columnas indicadoras de las palabras que aparecen en el texto, una columna por cada palabra. Este enfoque tan simple tiene el problema de que la misma palabra pero en singular/plural o un verbo en distintos tiempos verbales serían identificados como palabras distintas, y tendríamos demasiadas palabras y pocas repeticiones. Para evitar esto, una técnica común es utilizar las raíces de las palabras (stems). Por ejemplo, las raíces de las palabras ”comida” y ”comer” será ”com” (esto puede generar algunas identificaciones incorrectas pero en general funciona bastante bien). Para obtener las raíces de las palabras vamos a usar SnowballStemmer de la biblioteca nltk. Otro problema es que palabras que funcionan como conectores (por ejemplo because, any, above, ...) no aportan información sobre el contenido del texto y es conveniente eliminarlas. En inglés, en el contexto de modelos de lenguaje, estas palabras se llaman stop words (son palabras que queremos frenar antes de procesar los textos). La biblioteca `nltk` provee un listado de stop words que podemos utilizar.\n"
      ],
      "metadata": {
        "id": "F_zzwBezpAQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 1.\n",
        "Cargar en un DataFrame `df_news` los datos del archivo noticias.csv.\n"
      ],
      "metadata": {
        "id": "IWwTB7V_my8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 2.\n",
        "Crear un DataFrame `df_sports` que contenga solo las noticias de la categoría deportes, y resetear los índices.\n"
      ],
      "metadata": {
        "id": "FEGfrNczm0DU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 3.\n",
        "Generar un DataFrame `df_sports_stems` que contenga una columna por cada stem que aparece en los contenidos de las noticias de deportes, excluyendo las stop words. Para cada noticia, cada columna indica la cantidad de veces que aparece esa palabra en la noticia. Pueden utilizar el siguiente código.\n"
      ],
      "metadata": {
        "id": "9zi3aHmwm1Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = SnowballStemmer(language = \"english\")\n",
        "# Generamos una lista con todos los stems de palabras del texto\n",
        "# (excluyendo stop words y palabras que tienen numeros o simbolos)\n",
        "\n",
        "def tokenize_and_stem(text) :\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stems = [stemmer.stem(token) for token in tokens if\n",
        "    (token.isalpha() and token not in stop_words)]\n",
        "    return stems\n",
        "vectorizer = CountVectorizer(analyzer = tokenize_and_stem)\n",
        "X = vectorizer.fit_transform(df_sports[\"content\"])\n",
        "df_sports_stems = pd.DataFrame(X.toarray() ,\n",
        "columns = vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "3F_l_AMsiYNr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "cda7fa46-a116-4ae1-962f-e39e2bc39219"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_sports' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-713100254e4a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_stem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m df_sports_stems = pd.DataFrame(X.toarray() ,\n\u001b[1;32m     22\u001b[0m columns = vectorizer.get_feature_names_out())\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_sports' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 4.\n",
        "Visualización. Para poder visualizar las noticias y ver si se forman grupos de noticias, calcular las dos primeras componentes principales $Z_1$ y $Z_2$ del DataFrame `df_sports_stems` y realizar un gráfico de dispersión de $Z_1$ vs. $Z_2$.\n"
      ],
      "metadata": {
        "id": "rxHdLxbsiaOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 5.\n",
        "¿Pueden observar grupos en esta visualización? ¿Cuál puede ser el problema?\n"
      ],
      "metadata": {
        "id": "5g65PkSpm3m1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**TF-IDF.** El enfoque bag-of-words es demasiado simple. Hay palabras que aparecen mucho en los textos (como por ejemplo ”today” o ”say”) que no sirven para relacionar artículos con temáticas parecidas. Queremos darle poco peso a estas palabras. Por el contrario, si una palabra aparece en solo 3 noticias, podemos imaginar que esas 3 noticias están relacionadas. Queremos darle más peso a palabras poco frecuentes.\n",
        "\n",
        "Una métrica usual que da muy buenos resultados es la llamada TF-IDF.\n",
        "El término TF (term frequency) es la frecuencia de la palabra en el texto, normalizada:\n",
        "\n",
        "$$\\text{TF}(t, d) = \\frac{\\text{cantidad de veces que la palabra } t \\text{ aparece en el texto } d}{||\\text{vector de frecuencias del documento } d||_2}$$\n",
        "\n",
        "donde el denominador es $\\sqrt{\\sum_{t′} f^2_{t′,d}} $,  la raíz cuadrada de la suma de los cuadrados de las frecuencias de todos los términos en el documento $d$ (similar a la cantidad total de palabras en el texto). El término IDF (inverse document frequency) es el logaritmo del cociente entre el total de documentos y el número de documentos que contienen un término, más uno:\n",
        "\n",
        "$$\n",
        "\\text{IDF}(t, D) = log\\left(\\frac{\\text{número total de documentos en el corpus } D}{ \\text{cantidad de documentos en el corpus } D \\text{ que tienen la palabra }t}\\right) + 1\n",
        "$$\n",
        "\n",
        "Finalmente, definimos\n",
        "$$\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) × \\text{IDF}(t, D)$$\n",
        "Esta métrica no es difícil de calcular a partir de las variables que ya calculamos, pero podemos también hacerlo automáticamente a través del comando `TfidfVectorizer`.\n"
      ],
      "metadata": {
        "id": "yU-c3Hm0o92Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 6.\n",
        "Generar un DataFrame `df_sports_tfidf` que contenga una columna por cada stem que aparece en los contenidos de las noticias de deportes. Para cada noticia, cada columna indica el valor de TF-IDF. Pueden utilizar el siguiente código.\n"
      ],
      "metadata": {
        "id": "ZWlxFEdDm5M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text vectorization using TF - IDF\n",
        "vectorizer = TfidfVectorizer(analyzer=tokenize_and_stem, smooth_idf=False)\n",
        "X = vectorizer.fit_transform(df_sports[\"content\"])\n",
        "df_sports_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "L5FzGrSIluF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### O1. (Opcional.)\n",
        "Para la raíz abandon, calcular mediante funciones usuales el valor de TF-IDF para la noticia deportiva 246 (con título Real will finish abandoned match). Comparar el valor en la tabla generada con el valor que obtuvieron a mano en el ítem anterior.\n"
      ],
      "metadata": {
        "id": "qjaKigy5lv6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 7. Visualización.\n",
        "Calcular las dos primeras componentes principales $Z_1$ y $Z_2$ del DataFrame\n",
        "`df_sports_tfidf` y realizar un gráfico de dispersión de $Z_1$ vs. $Z_2$. ¿Pueden observar agrupamientos en esta visualización?\n"
      ],
      "metadata": {
        "id": "RK_1QYpao8s8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 8. Clustering.\n",
        "Realizar un agrupamiento de los datos por algún método de clustering entre los\n",
        "vistos en clase (K-means o DB-scan). Utilizar todas las columnas de datos (no las componentes principales), seleccionado los hiperparámetros por algún mecanismo que consideren apropiado.\n"
      ],
      "metadata": {
        "id": "DlLAzty-o8AH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 9.\n",
        "Colorear los puntos del gráfico de dispersión de PCA según las etiquetas de los clusters. ¿Quedan los puntos de cada cluster ubicados cerca en esta visualización?\n"
      ],
      "metadata": {
        "id": "PPz3hLnAo7Ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 10. Identificación de temáticas.\n",
        "Queremos identificar las temáticas de algunos de estos clusters. Para esto pueden utilizar cualquier estrategia que consideren apropiada. Por ejemplo,\n",
        "- ver los títulos o contenidos de las noticias en algunos clusters.\n",
        "- si algunas de las componentes principales distingue clusters, ver qué palabras tienen peso en esa componente. Por ejemplo, si vemos que un cluster corresponde a noticias con coordenada $Z_1$ alta, analizamos en las coordenadas de la dirección principal correspondiente qué palabras tienen el mayor peso positivo.\n",
        "\n",
        "Algunos clusters pueden no tener una temática clara. Identificar temáticas en al menos 2 clusters del agrupamiento.\n"
      ],
      "metadata": {
        "id": "aemZq3jto6iW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Clasificación [4 pts.]\n"
      ],
      "metadata": {
        "id": "7h0nKXfno5Qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Trabajamos ahora con toda la base de datos. Queremos predecir la categoría de cada artículo según el contenido del texto. Nuevamente vamos a utilizar la métrica TD-IDF y suponer que noticias cercanas en esta métrica corresponden a noticias de temática similar.\n"
      ],
      "metadata": {
        "id": "lNWbzapgpL78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 11.\n",
        "Generar un DataFrame `df_news_tfidf` que contenga una columna por cada stem que aparece en los contenidos de todas las noticias. Para cada noticia, cada columna indica el valor de TF-IDF (utilicen el mismo código de antes para generar este DataFrame).\n"
      ],
      "metadata": {
        "id": "PjXwRtTGo4Y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 12. Visualización.\n",
        "Calcular las dos primeras componentes principales $Z_1$ y $Z_2$ del DataFrame\n",
        "`df_news_tfidf` y realizar un gráfico de dispersión de $Z_1$ vs. $Z_2$, coloreando cada punto según la categoría del artículo.\n",
        "¿Quedan las noticias de cada categoría ubicadas cerca en esta visualización?\n"
      ],
      "metadata": {
        "id": "6g_3qMVMo3Sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### O2. (Opcional.)\n",
        "¿Qué palabras tienen peso positivo alto y peso negativo alto en estas dos compo-\n",
        "nentes?\n"
      ],
      "metadata": {
        "id": "HA2VVsHbo2c4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 13.\n",
        "Predecir la categoría de cada artículo utilizando KNN. Para esto:\n",
        " 1. separar el conjunto total de datos en un 80% para entrenamiento y un 20% para testeo\n",
        " 2. seleccionar el valor de K (entre números impares del 1 al 19 ambos inclusive) por algún esquema de validación apropiado en el conjunto de entrenamiento (*train-validation*, *cross-validation*, *leave-one-out*),\n",
        " 3. para el valor hallado, calcular la precisión del método (porcentaje de aciertos) en el conjunto de testeo.\n"
      ],
      "metadata": {
        "id": "Bw6ztfCro1a5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### O3. (Opcional.) Maldición de la dimensionalidad.\n",
        "Vimos que al utilizar muchas variables explicativas podemos caer en la maldición de la dimensionalidad. Una estrategia posible para evitar esto es usar las componentes principales para clasificación. ¿Cuántas componentes tenemos que tomar si queremos que esas componentes expliquen al menos un 10% de la varianza total? Definir un DataFrame $Z$ que contenga esas componentes y repetir el ejercicio anterior. ¿Mejoró la precisión del método?\n"
      ],
      "metadata": {
        "id": "1c6kOkHNo0dX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Clasificación utilizando títulos de las noticias [2 pts.]\n"
      ],
      "metadata": {
        "id": "NyX8D8vvozWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Hasta ahora utilizamos el contenido de las noticias para clasificar. En estas noticias los títulos son muy breves y resulta más difícil utilizarlos para clasificar.\n"
      ],
      "metadata": {
        "id": "vB8vNzSZpN0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 14.\n",
        "Generar un DataFrame `df_titles_tfidf` que contenga una columna por cada stem que aparece en los títulos de las noticias. Para cada noticia, cada columna indica el valor de TF-IDF.\n"
      ],
      "metadata": {
        "id": "kqeDgbF1oye0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### O4. (Opcional.) Aplicar KNN como en la sección anterior para clasificación de la categoría utilizando solo los títulos de las noticias. ¿Qué porcentaje de aciertos obtenemos?\n"
      ],
      "metadata": {
        "id": "5MxxcW25ov07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Vamos a enfocarnos ahora en predecir correctamente qué artículos corresponden a la categoría de entretenimiento.\n"
      ],
      "metadata": {
        "id": "lfrhsAs3oxEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 15.\n",
        "Definir una nueva serie entretenimiento que tome valor 1 si la categoría es entretenimiento y 0 si no. Aplicar KNN como en la sección anterior para clasificación de la categoría entrenimiento utilizando solo los títulos de las noticias (es decir, para predecir la variable entretenimiento). ¿Qué porcentaje de aciertos obtenemos?\n"
      ],
      "metadata": {
        "id": "M-Vkfjrcou9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 16.\n",
        "Proponer otro modelo de clasificación para predecir el valor de esta variable. Puede ser un modelo de regresión lineal, logística, red neuronal o cualquier otro modelo.\n"
      ],
      "metadata": {
        "id": "3RwpcYIJouOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ejercicio 17.\n",
        "Utilizando los mismos conjuntos de entrenamiento y testeo de la sección anterior, calcular los parámetros e hiperparámetros del modelo en el conjunto de entrenamiento y calcular el porcentaje de aciertos en el conjunto de testeo. Comparar con KNN. (No se preocupen si no obtienen un porcentaje mayor al de KNN, mientras sigan los pasos correctos para ajustar el modelo.)"
      ],
      "metadata": {
        "id": "DjmiY4AoosYp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mgs8IfD8GIl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}